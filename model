## 이미지 분류 순서 : 모델 구성 -> 모델 훈련 -> 예측

## train 시킬 모델 구성하는 작업

from PIL import Image
import os, glob, numpy as np
from sklearn.model_selection import train_test_split

caltech_dir = "C:/data/project/train"
categories = ['cup-cakes', 'donuts', 'french-fries', 'gyoza', 'hamberger', 'ice-cream', 'macarons', 'pizza', 'ramen',
              'risotto', 'sandwich', 'spaghetti-bolognese', 'spaghetti-carbonara', 'sushi', 'takoyaki', 'tiramisu', 'waffles']
nb_classes = len(categories)
print(nb_classes)
print(type(nb_classes))
## 카테고리 별로 돌면서 label을 0으로 초기화
image_w = 64
image_h = 64

pixels = image_h * image_w * 3

X = []
y = []

for idx, cat in enumerate(categories):
    
    #one-hot 돌리기.
    label = [0 for i in range(nb_classes)]
    label[idx] = 1

    image_dir = caltech_dir + "/" + cat
    files = glob.glob(image_dir+"/*.jpg")
    print(cat, " 파일 길이 : ", len(files))
    for i, f in enumerate(files):
        img = Image.open(f)
        img = img.convert("RGB")
        img = img.resize((image_w, image_h))
        data = np.asarray(img)

        X.append(data)
        y.append(label)

        if i % 700 == 0:
            print(cat, " : ", f)
            
X = np.array(X)
y = np.array(y)
#1 0 0 0 이면 apple_pie
#0 1 0 0 이면 baklava 이런식

# numpy 라이브러리를 이용하여 저장
X_train, X_test, y_train, y_test = train_test_split(X, y)
xy = (X_train, X_test, y_train, y_test)
np.save("C:/data/project/multi_image_data.npy", xy)

print("ok", len(y))

import os, glob, numpy as np
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout
from keras.callbacks import EarlyStopping, ModelCheckpoint
import matplotlib.pyplot as plt
import keras.backend.tensorflow_backend as K
import tensorflow as tf

## tensorflow의 특성상 모든 gpu 메모리를 할당받은 후 이용하기 때문에 메모리 낭비 심함
## allow_groth 옵션을 통해 작은 메모리에서 큰 메모리로 사용량에 따라 늘려가게 함
config = tf.compat.v1.ConfigProto()
config.gpu_options.allow_growth = True
session = tf.compat.v1.Session(config=config)

## numpy를 불러와서 train시키게 함
X_train, X_test, y_train, y_test = np.load("C:/data/project/multi_image_data.npy", allow_pickle=True)
print(X_train.shape)
print(X_train.shape[0])

categories = ['cup-cakes', 'donuts', 'french-fries', 'gyoza', 'hamberger', 'ice-cream', 'macarons', 'pizza', 'ramen',
              'risotto', 'sandwich', 'spaghetti-bolognese', 'spaghetti-carbonara', 'sushi', 'takoyaki', 'tiramisu', 'waffles']
nb_classes = len(categories)

# 데이터 타입을 일반화 시키기
X_train = X_train.astype(float) / 255
X_test = X_test.astype(float) / 255

## 신경망 만들기
with K.tf_ops.device('/device:GPU:0'):
    model = Sequential()
    model.add(Conv2D(32, (3,3), padding="same", input_shape=X_train.shape[1:], activation='relu'))
    model.add(MaxPooling2D(pool_size=(2,2)))
    model.add(Dropout(0.25))
    
    model.add(Conv2D(64, (3,3), padding="same", activation='relu'))
    model.add(MaxPooling2D(pool_size=(2,2)))
    model.add(Dropout(0.25))
    
    model.add(Flatten())
    model.add(Dense(256, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(17, activation='softmax'))
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    model_dir = './model'
    
    if not os.path.exists(model_dir):
        os.mkdir(model_dir)
    
    model_path = model_dir + '/multi_img_classification.model'
    checkpoint = ModelCheckpoint(filepath=model_path , monitor='val_loss', verbose=1, save_best_only=True)
    early_stopping = EarlyStopping(monitor='val_loss', patience=6)
    
    
## 모델 요약
model.summary()

history = model.fit(X_train, y_train, batch_size=32, epochs=15, validation_data=(X_test, y_test))
model.save('C:/data/project/train/food_model.h5')
